# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             LexJadeLM6 Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
import math
from transformers import PretrainedConfig
class LexJadeLM6Config(PretrainedConfig):
    """
    LexJadeLM6 ç³»åˆ—æ¨¡å‹çš„é…ç½®ç±»ã€‚
    éµå¾ªé’ˆå¯¹ä¸åŒè§„æ¨¡è¿›è¡Œç‹¬ç«‹ç»“æ„ä¼˜åŒ–çš„åŸåˆ™ã€‚
    """
    model_type = "lexjadelm6"
    def __init__(
        self,
        # åŸºç¡€æ¨¡å‹é…ç½®
        vocab_size: int = 15500, # é»˜è®¤ä¸º600Mæ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°
        hidden_size: int = 1024, # é»˜è®¤ä¸º600Mæ¨¡å‹ (Ultra) çš„éšè—å±‚å¤§å°
        intermediate_size: int = None, # é€šå¸¸ç”± hidden_size å’Œé—¨æ§æœºåˆ¶æ¨å¯¼
        num_hidden_layers: int = 12, # ç¤ºä¾‹å±‚æ•°
        num_attention_heads: int = 12, # é»˜è®¤ä¸º600Mæ¨¡å‹ (Ultra) çš„å¤´æ•°
        num_key_value_heads: int = 4, # ä½¿ç”¨GQAï¼Œé”®å€¼å¤´æ•°é€šå¸¸å°‘äºæŸ¥è¯¢å¤´æ•°
        hidden_act: str = "silu", # SLMå¸¸ç”¨æ¿€æ´»å‡½æ•°
        max_position_embeddings: int = 4096, # ç¤ºä¾‹æœ€å¤§ä½ç½®ç¼–ç 
        initializer_range: float = 0.02,
        rms_norm_eps: float = 1e-5, # RMSNorm epsilon
        use_cache: bool = True,
        pad_token_id: int = 0,
        bos_token_id: int = 1,
        eos_token_id: int = 2,
        pretraining_tp: int = 1,
        tie_word_embeddings: bool = False,
        rope_theta: float = 10000.0,
        rope_scaling: dict = None,
        attention_bias: bool = False,
        attention_dropout: float = 0.0,
        mlp_bias: bool = False,
        # Dropout
        dropout: float = 0.0,
        # æ»‘åŠ¨çª—å£æ³¨æ„åŠ› (é€‚ç”¨äº Tiny/Flash/Cube/Large)
        sliding_window: int = None, # e.g., 1024 for Tiny/Flash/Cube/Large
        global_attn_every_n_layers: int = None, # e.g., 5 for Tiny/Flash/Cube/Large
        ####################################################
        # MoE é…ç½® (é€‚ç”¨äº Extreme-M / Ultra-M)
        # å½“ use_moe ä¸º False æ—¶ï¼Œä»¥ä¸‹é…ç½®æ— æ•ˆ
        ####################################################
        use_moe: bool = False,
        moe_layer_freq: int = 1, # MoEå±‚å‡ºç°çš„é¢‘ç‡
        num_experts_per_tok: int = 4, # æ¯ä¸ªtokenæ¿€æ´»çš„ä¸“å®¶æ•° K (æ¥è‡ªæ–‡æ¡£: 6è·¯ç”±ä¸“å®¶ï¼Œæ¯tokenæ¿€æ´»4ä¸ª)
        n_routed_experts: int = 6, # è·¯ç”±ä¸“å®¶æ€»æ•° N (æ¥è‡ªæ–‡æ¡£)
        n_shared_experts: int = 1, # å…±äº«ä¸“å®¶æ•°é‡ (æ¥è‡ªæ–‡æ¡£)
        expert_capacity_factor: float = 1.35, # ä¸“å®¶å®¹é‡å› å­ (æ¥è‡ªæ–‡æ¡£)
        moe_aux_loss_alpha: float = 0.05, # MoEè¾…åŠ©æŸå¤±ç³»æ•° (åŠ¨æ€å¢é•¿çš„æœ€ç»ˆå€¼, æ¥è‡ªæ–‡æ¡£)
        moe_aux_loss_alpha_init: float = 0.01, # MoEè¾…åŠ©æŸå¤±ç³»æ•°åˆå§‹å€¼ (æ¥è‡ªæ–‡æ¡£)
        moe_scoring_func: str = 'softmax', # ä¸“å®¶è¯„åˆ†å‡½æ•°
        moe_norm_topk_prob: bool = True, # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡
        moe_seq_aux: bool = True, # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«è®¡ç®—è¾…åŠ©æŸå¤±
        # MoGE ç‰¹å®šé…ç½® (ä¸“å®¶åˆ†ç»„) - æ–‡æ¡£å»ºè®® M = N / K, ä½†éœ€ä¸ºæ•´æ•°
        moe_num_expert_groups: int = None, # ä¸“å®¶ç»„æ•° M, ä¾‹å¦‚ 6ä¸“å®¶4æ¿€æ´»ï¼Œå¯èƒ½åˆ†2ç»„(æ¯ç»„3ä¸“å®¶ï¼Œæ¯ç»„æ¿€æ´»2ä¸ª)
        # ä¸“å®¶éšè—å±‚å¤§å° (é€šå¸¸æ¯”æ™®é€šFFNå¤§ä»¥è¡¥å¿ç¨€ç–æ€§) - æ–‡æ¡£å»ºè®®å¢åŠ çº¦15%
        moe_expert_hidden_size_factor: float = 1.15, # ä¸“å®¶å±‚éšè—å¤§å°ç›¸å¯¹äºæ ‡å‡†FFNçš„å€æ•°
        ####################################################
        # PEMA å¾®è°ƒä¼˜åŒ– (é…ç½®ä¿¡æ¯é€šå¸¸åœ¨è®­ç»ƒ/å¾®è°ƒè„šæœ¬ä¸­å¤„ç†)
        ####################################################
        use_pema: bool = False, # å¾®è°ƒæ—¶æ˜¯å¦å¯ç”¨PEMA
        **kwargs,
    ):
        # --- åŸºç¡€å‚æ•° ---
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        # è®¡ç®—æˆ–è®¾ç½® intermediate_size
        if intermediate_size is None:
            # å¸¸è§çš„é—¨æ§FFNä¸­é—´å¤§å°è®¡ç®—æ–¹å¼
            intermediate_size = int(8 * self.hidden_size / 3)
            # å‘ä¸Šå–æ•´åˆ°256çš„å€æ•°ï¼Œç¬¦åˆSLMå®è·µ
            self.intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)
        else:
            self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        # æ£€æŸ¥ head_dim æ˜¯å¦æ•´é™¤
        if self.hidden_size % self.num_attention_heads != 0:
             raise ValueError(
                 f"`hidden_size` must be divisible by `num_attention_heads` (got `hidden_size`: {self.hidden_size}"
                 f" and `num_attention_heads`: {self.num_attention_heads})."
             )
        self.head_dim = self.hidden_size // self.num_attention_heads
        self.num_key_value_heads = num_key_value_heads
        # æ£€æŸ¥ GQA é…ç½®æ˜¯å¦æœ‰æ•ˆ
        if self.num_key_value_heads > self.num_attention_heads:
            raise ValueError(
                f"`num_key_value_heads` ({self.num_key_value_heads}) cannot be larger than `num_attention_heads` "
                f"({self.num_attention_heads})."
            )
        if self.num_attention_heads % self.num_key_value_heads != 0:
            raise ValueError(
                f"`num_attention_heads` ({self.num_attention_heads}) must be divisible by "
                f"`num_key_value_heads` ({self.num_key_value_heads})."
            )
        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads
        if hidden_act != "silu":
             print(f"è­¦å‘Š: LexJade6 é€šå¸¸ä½¿ç”¨ 'silu' æ¿€æ´»å‡½æ•°, å¾—åˆ°çš„æ˜¯ '{hidden_act}'.")
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.use_cache = use_cache
        self.pretraining_tp = pretraining_tp
        self.tie_word_embeddings = tie_word_embeddings
        self.rope_theta = rope_theta
        self.rope_scaling = rope_scaling
        self.attention_bias = attention_bias
        self.attention_dropout = attention_dropout
        self.mlp_bias = mlp_bias
        # --- æ³¨æ„åŠ›æœºåˆ¶ç‰¹å®šå‚æ•° ---
        self.sliding_window = sliding_window
        self.global_attn_every_n_layers = global_attn_every_n_layers
        # --- MoE å‚æ•° ---
        self.use_moe = use_moe
        self.moe_layer_freq = moe_layer_freq
        self.num_experts_per_tok = num_experts_per_tok
        self.n_routed_experts = n_routed_experts
        self.n_shared_experts = n_shared_experts
        self.expert_capacity_factor = expert_capacity_factor
        self.moe_aux_loss_alpha = moe_aux_loss_alpha
        self.moe_aux_loss_alpha_init = moe_aux_loss_alpha_init
        self.moe_scoring_func = moe_scoring_func
        self.moe_norm_topk_prob = moe_norm_topk_prob
        self.moe_seq_aux = moe_seq_aux
        self.moe_num_expert_groups = moe_num_expert_groups
        self.moe_expert_hidden_size_factor = moe_expert_hidden_size_factor
        # --- å…¶ä»–å‚æ•° ---
        self.use_pema = use_pema
        self.dropout = dropout
        super().__init__(
            pad_token_id=pad_token_id,
            bos_token_id=bos_token_id,
            eos_token_id=eos_token_id,
            tie_word_embeddings=tie_word_embeddings,
            **kwargs,
        )
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                          LexJadeLM6 Model Variants Configurations
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
# --- LexJade6-Tiny (5M) ---
class LexJade6TinyConfig(LexJadeLM6Config):
    """LexJade6-Tiny (5Må‚æ•°) é…ç½®"""
    def __init__(self, **kwargs):
        super().__init__(
            vocab_size=10000,
            hidden_size=256,
            num_hidden_layers=4, # ç¤ºä¾‹å±‚æ•°ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
            num_attention_heads=4,
            num_key_value_heads=2, # GQA
            intermediate_size=512, # ç¤ºä¾‹ï¼Œå®é™…ä¼šç”±åŸºç±»è®¡ç®—è¦†ç›–
            sliding_window=1024, # å¯ç”¨æ»‘åŠ¨çª—å£
            global_attn_every_n_layers=5, # æ»‘åŠ¨çª—å£é…ç½®
            use_moe=False,
            **kwargs,
        )
# --- LexJade6-Flash (25M) ---
class LexJade6FlashConfig(LexJadeLM6Config):
    """LexJade6-Flash (25Må‚æ•°) é…ç½®"""
    def __init__(self, **kwargs):
        super().__init__(
            vocab_size=10000,
            hidden_size=256,
            num_hidden_layers=8, # ç¤ºä¾‹å±‚æ•°
            num_attention_heads=4,
            num_key_value_heads=2, # GQA
            intermediate_size=512, # ç¤ºä¾‹
            sliding_window=1024, # å¯ç”¨æ»‘åŠ¨çª—å£
            global_attn_every_n_layers=5, # æ»‘åŠ¨çª—å£é…ç½®
            use_moe=False,
            **kwargs,
        )
# --- LexJade6-Cube (50M) ---
class LexJade6CubeConfig(LexJadeLM6Config):
    """LexJade6-Cube (50Må‚æ•°) é…ç½®"""
    def __init__(self, **kwargs):
        super().__init__(
            vocab_size=10000,
            hidden_size=512,
            num_hidden_layers=8, # ç¤ºä¾‹å±‚æ•°
            num_attention_heads=8,
            num_key_value_heads=4, # GQA
            intermediate_size=1024, # ç¤ºä¾‹
            sliding_window=1024, # å¯ç”¨æ»‘åŠ¨çª—å£
            global_attn_every_n_layers=5, # æ»‘åŠ¨çª—å£é…ç½®
            use_moe=False,
            **kwargs,
        )
# --- LexJade6-Large (100M) ---
class LexJade6LargeConfig(LexJadeLM6Config):
    """LexJade6-Large (100Må‚æ•°) é…ç½®"""
    def __init__(self, **kwargs):
        super().__init__(
            vocab_size=10000,
            hidden_size=512,
            num_hidden_layers=12, # ç¤ºä¾‹å±‚æ•°
            num_attention_heads=8,
            num_key_value_heads=4, # GQA
            intermediate_size=1024, # ç¤ºä¾‹
            sliding_window=1024, # å¯ç”¨æ»‘åŠ¨çª—å£
            global_attn_every_n_layers=5, # æ»‘åŠ¨çª—å£é…ç½®
            use_moe=False,
            **kwargs,
        )
# --- LexJade6-Extreme (300M) ---
class LexJade6ExtremeConfig(LexJadeLM6Config):
    """LexJade6-Extreme (300Må‚æ•°) é…ç½®"""
    def __init__(self, **kwargs):
        super().__init__(
            vocab_size=11400,
            hidden_size=768,
            num_hidden_layers=16, # ç¤ºä¾‹å±‚æ•°
            num_attention_heads=8,
            num_key_value_heads=4, # GQA
            intermediate_size=2048, # ç¤ºä¾‹
            use_moe=False, # å¯†é›†æ¨¡å‹
            **kwargs,
        )
# --- LexJade6-Ultra (600M) ---
class LexJade6UltraConfig(LexJadeLM6Config):
    """LexJade6-Ultra (600Må‚æ•°) é…ç½®"""
    def __init__(self, **kwargs):
        super().__init__(
            vocab_size=15500,
            hidden_size=1024,
            num_hidden_layers=16, # ç¤ºä¾‹å±‚æ•°
            num_attention_heads=12,
            num_key_value_heads=4, # GQA
            intermediate_size=2816, # ç¤ºä¾‹ (å‚è€ƒQwen2ç»“æ„)
            use_moe=False, # å¯†é›†æ¨¡å‹
            **kwargs,
        )
# --- LexJade6-Extreme-M (300M-MoE) ---
class LexJade6ExtremeMConfig(LexJadeLM6Config):
    """LexJade6-Extreme-M (300M-MoEå‚æ•°) é…ç½®"""
    def __init__(self, **kwargs):
        super().__init__(
            vocab_size=11400,
            hidden_size=768,
            num_hidden_layers=16, # ç¤ºä¾‹å±‚æ•°
            num_attention_heads=8,
            num_key_value_heads=4, # GQA
            intermediate_size=2048, # åŸºç¡€FFNä¸­é—´å¤§å°ç¤ºä¾‹
            use_moe=True, # å¯ç”¨MoE
            moe_layer_freq=1, # ç¤ºä¾‹ï¼šæ¯å±‚éƒ½æ˜¯MoE
            num_experts_per_tok=4, # æ–‡æ¡£é…ç½®
            n_routed_experts=6, # æ–‡æ¡£é…ç½®
            n_shared_experts=1, # æ–‡æ¡£é…ç½®
            expert_capacity_factor=1.35, # æ–‡æ¡£é…ç½®
            moe_aux_loss_alpha=0.05, # æ–‡æ¡£é…ç½®
            moe_aux_loss_alpha_init=0.01, # æ–‡æ¡£é…ç½®
            moe_num_expert_groups=2, # ç¤ºä¾‹åˆ†ç»„ (6ä¸“å®¶åˆ†2ç»„)
            moe_expert_hidden_size_factor=1.15, # æ–‡æ¡£é…ç½®
            **kwargs,
        )
# --- LexJade6-Ultra-M (600M-MoE) ---
class LexJade6UltraMConfig(LexJadeLM6Config):
    """LexJade6-Ultra-M (600M-MoEå‚æ•°) é…ç½®"""
    def __init__(self, **kwargs):
        super().__init__(
            vocab_size=15500,
            hidden_size=1024,
            num_hidden_layers=16, # ç¤ºä¾‹å±‚æ•°
            num_attention_heads=12,
            num_key_value_heads=4, # GQA
            intermediate_size=2816, # åŸºç¡€FFNä¸­é—´å¤§å°ç¤ºä¾‹
            use_moe=True, # å¯ç”¨MoE
            moe_layer_freq=1, # ç¤ºä¾‹ï¼šæ¯å±‚éƒ½æ˜¯MoE
            num_experts_per_tok=4, # æ–‡æ¡£é…ç½®
            n_routed_experts=6, # æ–‡æ¡£é…ç½®
            n_shared_experts=1, # æ–‡æ¡£é…ç½®
            expert_capacity_factor=1.35, # æ–‡æ¡£é…ç½®
            moe_aux_loss_alpha=0.05, # æ–‡æ¡£é…ç½®
            moe_aux_loss_alpha_init=0.01, # æ–‡æ¡£é…ç½®
            moe_num_expert_groups=2, # ç¤ºä¾‹åˆ†ç»„ (6ä¸“å®¶åˆ†2ç»„)
            moe_expert_hidden_size_factor=1.15, # æ–‡æ¡£é…ç½®
            **kwargs,
        )
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             LexJadeLM6 Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
import math
import torch
from torch import nn
from transformers.activations import ACT2FN
from typing import Optional, Tuple, List, Union
import torch.nn.functional as F
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
# --- ä¿®å¤ç‚¹ 1: å¯¼å…¥ BaseModelOutputWithPast ---
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
# --- ä¿®å¤ç‚¹ 1 ç»“æŸ ---
class RMSNorm(torch.nn.Module):
    """RMSNorm å½’ä¸€åŒ–å±‚ï¼Œè®¡ç®—æ›´è½»é‡"""
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    def _norm(self, x):
        # è®¡ç®—å‡æ–¹æ ¹å¹¶å–å€’æ•°
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
    def forward(self, x):
        # ä¿æŒè¾“å…¥ç±»å‹ä¸€è‡´
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):
    """
    é¢„è®¡ç®— RoPE (Rotary Position Embedding) çš„é¢‘ç‡å’Œå¤æ•°è¡¨ç¤ºã€‚
    """
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)
    freqs = torch.outer(t, freqs).float()
    # coså’Œsinç”¨äºæ—‹è½¬åµŒå…¥
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)
    return freqs_cos, freqs_sin
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=2): # --- ä¿®å¤ç‚¹ 2: é»˜è®¤å‚æ•°æ”¹ä¸º 2 ---
    """
    åº”ç”¨ RoPE åˆ°æŸ¥è¯¢(Q)å’Œé”®(K)å‘é‡ã€‚
    Args:
        q (torch.Tensor): æŸ¥è¯¢å‘é‡, å½¢çŠ¶ [..., seq_len, num_heads, head_dim]
        k (torch.Tensor): é”®å‘é‡, å½¢çŠ¶ [..., seq_len, num_heads, head_dim]
        cos (torch.Tensor): é¢„è®¡ç®—çš„ä½™å¼¦å€¼, å½¢çŠ¶ [..., seq_len, head_dim]
        sin (torch.Tensor): é¢„è®¡ç®—çš„æ­£å¼¦å€¼, å½¢çŠ¶ [..., seq_len, head_dim]
        position_ids (torch.Tensor, optional): ä½ç½®ID (æœªåœ¨æ­¤å‡½æ•°ä¸­ä½¿ç”¨, ä¿ç•™ä»¥å…¼å®¹æ€§).
        unsqueeze_dim (int): åœ¨å“ªä¸ªç»´åº¦ä¸Šå¯¹ cos/sin å¢åŠ ç»´åº¦ä»¥è¿›è¡Œå¹¿æ’­ã€‚å¯¹äº [B, S, H, D] è¾“å…¥ï¼Œåº”ä¸º 2ã€‚
    Returns:
        Tuple[torch.Tensor, torch.Tensor]: æ—‹è½¬åçš„ä½ç½®åµŒå…¥ q_embed å’Œ k_embedã€‚
    """
    def rotate_half(x):
        # å°†å‘é‡çš„ååŠéƒ¨åˆ†å–è´Ÿå¹¶äº¤æ¢å‰åä¸¤åŠ
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2 :]
        return torch.cat((-x2, x1), dim=-1)
    # åº”ç”¨æ—‹è½¬
    # q/k shape: [..., seq_len, num_heads, head_dim]
    # cos/sin shape: [..., seq_len, head_dim]
    # cos.unsqueeze(unsqueeze_dim) shape: [..., seq_len, 1, head_dim] (if unsqueeze_dim=2)
    # å¹¿æ’­: [..., seq_len, num_heads, head_dim] * [..., seq_len, 1, head_dim] -> OK
    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    å¤åˆ¶é”®å€¼å¤´ä»¥åŒ¹é…æŸ¥è¯¢å¤´çš„æ•°é‡ (ç”¨äºGQA)ã€‚
    ä¸ MiniMind å®ç°å®Œå…¨ä¸€è‡´ã€‚
    æœŸæœ›è¾“å…¥ x çš„å½¢çŠ¶ä¸º [batch_size, seq_len, num_key_value_heads, head_dim]ã€‚
    """
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    # ä½¿ç”¨ repeat_interleave åœ¨ head ç»´åº¦ä¸Šå¤åˆ¶
    # è¿™ä¸ MiniMind çš„åŸå§‹å®ç°å®Œå…¨ä¸€è‡´
    return torch.repeat_interleave(x, dim=2, repeats=n_rep)
class LexJadeAttention(nn.Module):
    """
    LexJade æ³¨æ„åŠ›å±‚ï¼Œæ”¯æŒ GQA å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ã€‚
    """
    def __init__(self, config: LexJadeLM6Config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = config.head_dim
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = config.num_key_value_groups
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)
        self.attention_dropout = config.attention_dropout
        # æ»‘åŠ¨çª—å£é…ç½®
        self.sliding_window = config.sliding_window if hasattr(config, 'sliding_window') else None
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Tuple[torch.Tensor, torch.Tensor], # cos, sin
        attention_mask: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()
        
        # --- ä¿®å¤ç‚¹ 1: ç¡®ä¿åœ¨ä½¿ç”¨ç¼“å­˜æ—¶åªå¤„ç†æœ€æ–° token ---
        # åœ¨ç”Ÿæˆæ¨¡å¼ä¸‹ï¼Œæ¯æ¬¡åªåº”å¤„ç†ä¸€ä¸ª token
        if past_key_value is not None and past_key_value[0] is not None and q_len > 1:
            # åªä¿ç•™æœ€åä¸€ä¸ª token (æœ€æ–° token)
            hidden_states = hidden_states[:, -1:, :]
            _, q_len, _ = hidden_states.size()  # æ›´æ–° q_len ä¸º 1
        # --- ä¿®å¤ç‚¹ 1 ç»“æŸ ---
        
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)
        
        # --- 1. é‡å¡‘ä¸º [B, S, Num_Heads/Num_KV_Heads, Head_Dim] ---
        # è¿™æ˜¯ä¸ MiniMind ä¿æŒä¸€è‡´çš„å…³é”®æ ¼å¼
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim)
        
        # --- 2. åº”ç”¨ RoPE ---
        cos, sin = position_embeddings
        # ç¡®ä¿ RoPE çš„ seq_len ä¸å½“å‰ q_len åŒ¹é…
        if cos.dim() == 2: # [seq_len, dim]
            cos = cos[:q_len]
            sin = sin[:q_len]
        elif cos.dim() == 3: # [bsz, seq_len, dim] or [1, seq_len, dim]
            cos = cos[:, :q_len, :]
            sin = sin[:, :q_len, :]
        
        # --- ä¿®å¤ç‚¹ 3: ä¿®æ”¹ unsqueeze_dim å‚æ•° ---
        # åº”ç”¨ RoPE
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, unsqueeze_dim=2) # --- ä¿®æ”¹ ---
        # --- ä¿®å¤ç‚¹ 3 ç»“æŸ ---
        
        # --- 3. KV Cache å¤„ç† ---
        if past_key_value is not None:
            # past_key_value åŒ…å« [past_key, past_value]
            past_key, past_value = past_key_value
            # åœ¨åºåˆ—ç»´åº¦ (dim=1) ä¸Šæ‹¼æ¥
            key_states = torch.cat([past_key, key_states], dim=1)
            value_states = torch.cat([past_value, value_states], dim=1)
        
        # ä¿å­˜æ–°çš„ KV ç¼“å­˜
        past_key_value = (key_states, value_states) if use_cache else None
        
        # --- 4. GQA: é‡å¤ K å’Œ V ä»¥åŒ¹é… Q çš„å¤´æ•° ---
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)
        
        # --- 5. è½¬ç½®ä¸º [B, Num_Heads, S, Head_Dim] ä»¥è¿›è¡Œæ³¨æ„åŠ›è®¡ç®— ---
        query_states = query_states.transpose(1, 2)
        key_states = key_states.transpose(1, 2)
        value_states = value_states.transpose(1, 2)
        
        # --- 6. æ ‡å‡†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (SDPA) ---
        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
        
        # --- 7. æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æ©ç å¤„ç† ---
        if self.sliding_window is not None and q_len > 1:
            current_len = key_states.shape[2]
            query_indices = torch.arange(q_len, device=attn_weights.device)
            key_indices = torch.arange(current_len, device=attn_weights.device)
            relative_positions = key_indices.unsqueeze(0) - query_indices.unsqueeze(1)
            sw_mask = (relative_positions < -self.sliding_window + 1) | (relative_positions > 0)
            sw_mask = sw_mask.unsqueeze(0).unsqueeze(0).expand(bsz, self.num_heads, q_len, current_len)
            attn_weights = attn_weights.masked_fill(sw_mask, torch.finfo(attn_weights.dtype).min)
        
        # --- 8. åº”ç”¨ä¼ å…¥çš„é€šç”¨æ³¨æ„åŠ›æ©ç  ---
        if attention_mask is not None:
            attention_mask = attention_mask.to(dtype=attn_weights.dtype)
            attn_weights = attn_weights + attention_mask
        
        # --- 9. softmax å½’ä¸€åŒ–å¹¶è®¡ç®—è¾“å‡º ---
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        attn_output = torch.matmul(attn_weights, value_states)
        
        # --- 10. è½¬ç½®å¹¶é‡å¡‘ ---
        attn_output = attn_output.transpose(1, 2).contiguous()
        # --- ä¿®å¤ç‚¹ 12: ä½¿ç”¨æ­£ç¡®çš„ç»´åº¦è¿›è¡Œé‡å¡‘ ---
        # ç¡®ä¿ä½¿ç”¨ num_heads * head_dim è€Œä¸æ˜¯ hidden_size
        # å› ä¸ºåœ¨æŸäº›é…ç½®ä¸­ (å¦‚ Tiny/Flash/Cube)ï¼Œnum_heads * head_dim != hidden_size
        # ä½†åœ¨ LexJade6-Flash ä¸­ï¼Œnum_heads * head_dim = hidden_size = 256
        attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.head_dim)
        # --- ä¿®å¤ç‚¹ 12 ç»“æŸ ---
        attn_output = self.o_proj(attn_output) # çº¿æ€§æŠ•å½±
        
        return attn_output, past_key_value
class LexJadeMLP(nn.Module):
    """
    LexJade å‰é¦ˆç½‘ç»œ (FFN)ï¼Œé‡‡ç”¨é—¨æ§ç»“æ„ (Gated FFN) å’Œ SiLU æ¿€æ´»å‡½æ•°ã€‚
    """
    def __init__(self, config: LexJadeLM6Config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        # é—¨æ§æŠ•å½±
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)
        # ä¸ŠæŠ•å½±
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)
        # ä¸‹æŠ•å½±
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)
        # æ¿€æ´»å‡½æ•°
        self.act_fn = ACT2FN[config.hidden_act]
    def forward(self, x):
        # Gated FFN: down_proj(act(gate_proj(x)) * up_proj(x))
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
class MoEGate(nn.Module):
    """
    MoE è·¯ç”±é—¨æ§ç½‘ç»œã€‚
    """
    def __init__(self, config: LexJadeLM6Config):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok
        self.n_routed_experts = config.n_routed_experts
        self.scoring_func = config.moe_scoring_func
        self.alpha = config.moe_aux_loss_alpha # æœ€ç»ˆçš„alphaå€¼
        self.seq_aux = config.moe_seq_aux
        self.norm_topk_prob = config.moe_norm_topk_prob
        self.gating_dim = config.hidden_size
        # é—¨æ§æƒé‡çŸ©é˜µ
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))
        self.reset_parameters()
    def reset_parameters(self) -> None:
        import torch.nn.init as init
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))
    def forward(self, hidden_states):
        bsz, seq_len, h = hidden_states.shape
        hidden_states = hidden_states.view(-1, h) # [B*S, H]
        logits = F.linear(hidden_states, self.weight, None) # [B*S, N]
        if self.scoring_func == 'softmax':
            scores = logits.softmax(dim=-1) # [B*S, N]
        else:
            raise NotImplementedError(f'ä¸æ”¯æŒçš„MoEé—¨æ§è¯„åˆ†å‡½æ•°: {self.scoring_func}')
        # é€‰æ‹© Top-K ä¸“å®¶
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False) # [B*S, K]
        if self.top_k > 1 and self.norm_topk_prob:
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
            topk_weight = topk_weight / denominator # æ ‡å‡†åŒ–
        aux_loss = 0.0
        # è®¡ç®—è¾…åŠ©è´Ÿè½½å‡è¡¡æŸå¤±
        if self.training and self.alpha > 0.0:
            scores_for_aux = scores
            aux_topk = self.top_k
            topk_idx_for_aux_loss = topk_idx.view(bsz, -1) # [B, S*K]
            if self.seq_aux:
                # åºåˆ—çº§åˆ«è¾…åŠ©æŸå¤±
                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1) # [B, S, N]
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                ce.scatter_add_(1, topk_idx_for_aux_loss,
                                torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)).div_(
                    seq_len * aux_topk / self.n_routed_experts)
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
            else:
                # Tokençº§åˆ«è¾…åŠ©æŸå¤±
                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)
                ce = mask_ce.float().mean(0)
                Pi = scores_for_aux.mean(0)
                fi = ce * self.n_routed_experts
                aux_loss = (Pi * fi).sum() * self.alpha
        return topk_idx, topk_weight, aux_loss
class MOEFeedForward(nn.Module):
    """
    MoE å‰é¦ˆç½‘ç»œï¼ŒåŒ…å«è·¯ç”±ä¸“å®¶å’Œå…±äº«ä¸“å®¶ã€‚
    """
    def __init__(self, config: LexJadeLM6Config):
        super().__init__()
        self.config = config
        # è®¡ç®—ä¸“å®¶ç½‘ç»œçš„ä¸­é—´å¤§å° (è¡¥å¿æ€§æ‰©å±•)
        base_intermediate_size = config.intermediate_size
        expert_intermediate_size = int(base_intermediate_size * config.moe_expert_hidden_size_factor)
        # ç¡®ä¿æ˜¯256çš„å€æ•°
        expert_intermediate_size = 256 * ((expert_intermediate_size + 256 - 1) // 256)
        # åˆ›å»ºè·¯ç”±ä¸“å®¶ (ä½¿ç”¨æ‰©å±•åçš„ä¸­é—´å¤§å°)
        expert_config = LexJadeLM6Config(**config.to_dict())
        expert_config.intermediate_size = expert_intermediate_size
        self.experts = nn.ModuleList([
            LexJadeMLP(expert_config) # ä½¿ç”¨ LexJadeMLP ä½œä¸ºä¸“å®¶
            for _ in range(config.n_routed_experts)
        ])
        # åˆ›å»ºé—¨æ§ç½‘ç»œ
        self.gate = MoEGate(config)
        # åˆ›å»ºå…±äº«ä¸“å®¶ (ä½¿ç”¨åŸºç¡€ä¸­é—´å¤§å°)
        if config.n_shared_experts > 0:
            shared_expert_config = LexJadeLM6Config(**config.to_dict())
            # å…±äº«ä¸“å®¶é€šå¸¸ä½¿ç”¨åŸºç¡€å¤§å°ï¼Œä½†ä¹Ÿå¯ä»¥è°ƒæ•´
            self.shared_experts = nn.ModuleList([
                LexJadeMLP(shared_expert_config)
                for _ in range(config.n_shared_experts)
            ])
    def forward(self, x):
        identity = x
        orig_shape = x.shape
        bsz, seq_len, _ = x.shape
        # ä½¿ç”¨é—¨æ§æœºåˆ¶é€‰æ‹©ä¸“å®¶
        topk_idx, topk_weight, aux_loss = self.gate(x)
        x = x.view(-1, x.shape[-1]) # [B*S, H]
        flat_topk_idx = topk_idx.view(-1) # [B*S*K]
        if self.training:
            # è®­ç»ƒæ¨¡å¼ï¼šå¤åˆ¶tokenå¹¶åˆ†é…ç»™ä¸“å®¶
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0) # [B*S*K, H]
            y = torch.empty_like(x, dtype=torch.float16)
            for i, expert in enumerate(self.experts):
                # å°†å¯¹åº”tokenåˆ†é…ç»™ä¸“å®¶i
                y[flat_topk_idx == i] = expert(x[flat_topk_idx == i]).to(y.dtype)
            # åŠ æƒæ±‚å’Œ
            y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1) # [B*S, H]
            y = y.view(*orig_shape) # [B, S, H]
        else:
            # æ¨ç†æ¨¡å¼ï¼šé«˜æ•ˆè·¯ç”± (å‚è€ƒMiniMind)
            y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)
        # æ·»åŠ å…±äº«ä¸“å®¶çš„è¾“å‡º
        if self.config.n_shared_experts > 0:
            for expert in self.shared_experts:
                y = y + expert(identity)
        self.aux_loss = aux_loss
        return y
    @torch.no_grad()
    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        """
        é«˜æ•ˆçš„MoEæ¨ç†è·¯ç”±ã€‚
        """
        expert_cache = torch.zeros_like(x)
        idxs = flat_expert_indices.argsort()
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0)
        token_idxs = idxs // self.config.num_experts_per_tok
        for i, end_idx in enumerate(tokens_per_expert):
            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]
            if start_idx == end_idx:
                continue
            expert = self.experts[i]
            exp_token_idx = token_idxs[start_idx:end_idx]
            expert_tokens = x[exp_token_idx]
            expert_out = expert(expert_tokens).to(expert_cache.dtype)
            expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]])
            expert_cache.scatter_add_(0, exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), expert_out)
        return expert_cache
class LexJadeDecoderLayer(nn.Module):
    """
    LexJade è§£ç å™¨å±‚ï¼ŒåŒ…å«è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œã€‚
    """
    def __init__(self, config: LexJadeLM6Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.self_attn = LexJadeAttention(config=config)
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨MoEå±‚
        use_moe_layer = (
            config.use_moe and
            (layer_idx % config.moe_layer_freq == 0) # ä¾‹å¦‚ï¼Œå¦‚æœ freq=1ï¼Œæ¯å±‚éƒ½æ˜¯MoE
        )
        if use_moe_layer:
            self.mlp = MOEFeedForward(config)
        else:
            self.mlp = LexJadeMLP(config)
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False,
        **kwargs,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        # è‡ªæ³¨æ„åŠ›
        hidden_states, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            past_key_value=past_key_value,
            use_cache=use_cache,
        )
        hidden_states = residual + hidden_states
        # å‰é¦ˆç½‘ç»œ (FFN æˆ– MoE)
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        outputs = (hidden_states,)
        if use_cache:
            outputs += (present_key_value,)
        return outputs # hidden_states, present_key_value
class LexJadeModel(PreTrainedModel):
    """
    LexJade ä¸»å¹²æ¨¡å‹ï¼ŒåŒ…å«åµŒå…¥å±‚ã€è§£ç å™¨å±‚å †å å’Œæœ€ç»ˆå½’ä¸€åŒ–å±‚ã€‚
    """
    config_class = LexJadeLM6Config
    def __init__(self, config: LexJadeLM6Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [LexJadeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.gradient_checkpointing = False
        # åˆå§‹åŒ–æƒé‡
        self.post_init()
        # RoPE é¢‘ç‡é¢„è®¡ç®—
        self.rope_theta = config.rope_theta
        freqs_cos, freqs_sin = precompute_freqs_cis(
            dim=config.head_dim, # æ³¨æ„æ˜¯ head_dim
            end=config.max_position_embeddings,
            theta=self.rope_theta
        )
        # æ³¨å†Œä¸º bufferï¼Œä¸å‚ä¸æ¢¯åº¦æ›´æ–°
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)
    def get_input_embeddings(self):
        return self.embed_tokens
    def set_input_embeddings(self, value):
        self.embed_tokens = value
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs
    ) -> Union[Tuple, BaseModelOutputWithPast]: # --- ä¿®å¤ç‚¹ 4: ä¿®æ”¹è¿”å›ç±»å‹æ³¨è§£ ---
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("ä¸èƒ½åŒæ—¶æŒ‡å®š input_ids å’Œ inputs_embeds")
        elif input_ids is not None:
            batch_size, seq_length = input_ids.shape[:2]
        elif inputs_embeds is not None:
            batch_size, seq_length = inputs_embeds.shape[:2]
        else:
            raise ValueError("å¿…é¡»æŒ‡å®š input_ids æˆ– inputs_embeds")
        if past_key_values is None:
            past_key_values = tuple([None] * len(self.layers))
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)
        # ä½ç½®IDå¤„ç†
        if position_ids is None:
             device = input_ids.device if input_ids is not None else inputs_embeds.device
             # è®¡ç®—å½“å‰åºåˆ—çš„èµ·å§‹ä½ç½® (è€ƒè™‘å†å²ç¼“å­˜)
             # past_key_values[0] æ˜¯ç¬¬ä¸€å±‚çš„past_kv, past_key_values[0][0] æ˜¯keyç¼“å­˜
             # keyç¼“å­˜çš„å½¢çŠ¶æ˜¯ [B, S_past, Num_KV_Heads, Head_Dim] (æ ¹æ®Attentionçš„å®ç°)
             past_seq_len = past_key_values[0][0].shape[1] if past_key_values[0] is not None and len(past_key_values[0]) > 0 else 0
             position_ids = torch.arange(
                 past_seq_len,
                 seq_length + past_seq_len,
                 dtype=torch.long,
                 device=device
             )
             position_ids = position_ids.unsqueeze(0) # [1, seq_len]
        # è·å– RoPE ä½ç½®åµŒå…¥ (æ ¹æ® position_ids åˆ‡ç‰‡)
        # freqs_cos/freqs_sin shape: [max_seq_len, dim]
        # position_ids shape: [batch_size, seq_len]
        # æˆ‘ä»¬éœ€è¦ gather æ­£ç¡®çš„ cos/sin å€¼
        cos = self.freqs_cos[position_ids] # [batch_size, seq_len, dim]
        sin = self.freqs_sin[position_ids] # [batch_size, seq_len, dim]
        # apply_rotary_pos_emb æœŸæœ› cos/sin æ˜¯ [*, seq_len, dim]ï¼Œè¿™å·²ç»æ»¡è¶³
        position_embeddings = (cos, sin) # [batch_size, seq_len, dim]
        # --- ä¿®å¤ç‚¹ 5: æ­£ç¡®å¤„ç† attention_maskï¼Œå…¼å®¹ past_key_values ---
        # å‡†å¤‡æ³¨æ„åŠ›æ©ç  (å¤„ç†å› æœæ€§å’Œpadding)
        # attention_mask (æ¥è‡ª tokenizer) å½¢çŠ¶é€šå¸¸æ˜¯ [B, S_input]
        # æˆ‘ä»¬éœ€è¦å°†å…¶å¤„ç†æˆé€‚ç”¨äºå½“å‰å‰å‘ä¼ æ’­çš„æ©ç 
        # å¦‚æœ use_cache=True (ç”Ÿæˆæ¨¡å¼)ï¼ŒS_total = past_seq_len + S_input
        # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå½¢çŠ¶ä¸º [B, 1, S_input, S_total] çš„æ©ç 
        if attention_mask is not None:
            # 1. ç¡®ä¿ attention_mask æ˜¯æ­£ç¡®çš„æ•°æ®ç±»å‹
            attention_mask = attention_mask.to(dtype=inputs_embeds.dtype) # [B, S_input]
            
            # 2. è·å–å½“å‰è¾“å…¥åºåˆ—é•¿åº¦ - ä¿®å¤ï¼šä» input_ids/inputs_embeds è·å–ï¼Œè€Œä¸æ˜¯ attention_mask
            # attention_mask çš„é•¿åº¦å¯èƒ½åŒ…å«å†å²ä¿¡æ¯ï¼Œè€Œæˆ‘ä»¬åªéœ€è¦å½“å‰è¾“å…¥çš„é•¿åº¦
            if input_ids is not None:
                batch_size, input_seq_len = input_ids.shape
            elif inputs_embeds is not None:
                batch_size, input_seq_len = inputs_embeds.shape[:2]
            else:
                raise ValueError("å¿…é¡»æŒ‡å®š input_ids æˆ– inputs_embeds")

            # 3. è®¡ç®—æ€»çš„ key/value åºåˆ—é•¿åº¦ (åŒ…æ‹¬å†å²ç¼“å­˜)
            # past_seq_len æ˜¯ä» past_key_values ä¸­è·å–çš„
            # past_key_values[0] æ˜¯ç¬¬ä¸€å±‚çš„ç¼“å­˜, [0] æ˜¯ key ç¼“å­˜
            # key ç¼“å­˜å½¢çŠ¶: [B, S_past, Num_KV_Heads, Head_Dim]
            past_seq_len = 0
            if past_key_values is not None and len(past_key_values) > 0 and past_key_values[0] is not None and len(past_key_values[0]) > 0:
                past_seq_len = past_key_values[0][0].shape[1] # [B, S_past, ...] -> S_past
            
            # åœ¨ç”Ÿæˆæ¨¡å¼ä¸‹ï¼Œinput_seq_len åº”ä¸º 1
            if past_seq_len > 0 and input_seq_len > 1:
                # ä¿®æ­£ï¼šåœ¨ç”Ÿæˆæ¨¡å¼ä¸‹ï¼Œåªåº”å¤„ç†æœ€æ–° token
                input_seq_len = 1
            
            total_seq_len = past_seq_len + input_seq_len # S_total
            
            # 4. åˆ›å»ºé€‚ç”¨äºå½“å‰è¾“å…¥çš„å› æœæ©ç  [S_input, S_input]
            # è¿™åªè€ƒè™‘å½“å‰è¾“å…¥å†…éƒ¨çš„å› æœå…³ç³»
            current_causal_mask = torch.tril(torch.ones((input_seq_len, input_seq_len), dtype=inputs_embeds.dtype, device=inputs_embeds.device))
            
            # 5. å¦‚æœæœ‰å†å²ç¼“å­˜ï¼Œéœ€è¦å…è®¸å½“å‰è¾“å…¥å…³æ³¨æ‰€æœ‰å†å²token
            # åˆ›å»ºä¸€ä¸ªå— [S_input, S_past] å¡«å……ä¸º1 (å…è®¸å…³æ³¨)
            if past_seq_len > 0:
                historical_ones = torch.ones((input_seq_len, past_seq_len), dtype=inputs_embeds.dtype, device=inputs_embeds.device)
                # å°†å†å²éƒ¨åˆ†å’Œå½“å‰å› æœéƒ¨åˆ†æ‹¼æ¥
                # æœ€ç»ˆçš„æœªåè½¬æ©ç å½¢çŠ¶ [S_input, S_total]
                combined_uninverted_mask = torch.cat([historical_ones, current_causal_mask], dim=1) # [S_input, S_total]
            else:
                # å¦‚æœæ²¡æœ‰å†å²ï¼Œåªä½¿ç”¨å½“å‰å› æœæ©ç 
                combined_uninverted_mask = current_causal_mask # [S_input, S_total] where S_total == S_input
            
            # 6. å°† combined_uninverted_mask æ‰©å±•åˆ° [B, S_input, S_total]
            expanded_uninverted_mask = combined_uninverted_mask.unsqueeze(0).expand(batch_size, -1, -1) # [B, S_input, S_total]
            
            # 7. åº”ç”¨åŸå§‹çš„ padding mask
            # attention_mask: [B, S_input] (1 for valid, 0 for pad)
            # expanded_uninverted_mask: [B, S_input, S_total]
            # æˆ‘ä»¬éœ€è¦å°† padding mask åº”ç”¨äº expanded_uninverted_mask çš„ç¬¬äºŒä¸ªç»´åº¦ (S_input)
            # expanded_padding_mask: [B, S_input, 1]
            expanded_padding_mask = attention_mask
            if past_seq_len > 0 and attention_mask.shape[1] > input_seq_len:
                # åœ¨ç”Ÿæˆæ¨¡å¼ä¸‹ï¼Œåªå–æœ€åä¸€ä¸ª token çš„ padding mask
                expanded_padding_mask = attention_mask[:, -input_seq_len:]
            expanded_padding_mask = expanded_padding_mask.unsqueeze(-1) # [B, S_input, 1]
            
            # å¹¿æ’­ç›¸ä¹˜: [B, S_input, 1] * [B, S_input, S_total] -> [B, S_input, S_total]
            combined_mask = expanded_padding_mask * expanded_uninverted_mask # [B, S_input, S_total]
            
            # 8. è½¬æ¢ä¸º inverted å½¢å¼ (0 for valid, min for masked)
            inverted_mask = 1.0 - combined_mask
            inverted_mask = inverted_mask.masked_fill(
                inverted_mask.to(torch.bool),
                torch.finfo(inputs_embeds.dtype).min
            )
            
            # 9. æ‰©å±•ç»´åº¦ä»¥åŒ¹é…æ³¨æ„åŠ›æƒé‡çš„æœŸæœ›è¾“å…¥ [B, 1, S_input, S_total]
            attention_mask = inverted_mask.unsqueeze(1) # [B, 1, S_input, S_total]
        # --- ä¿®å¤ç‚¹ 5 ç»“æŸ ---
        hidden_states = inputs_embeds
        # è§£ç å™¨å±‚å¾ªç¯
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        next_decoder_cache = () if use_cache else None
        for idx, decoder_layer in enumerate(self.layers):
            if output_hidden_states:
                all_hidden_states += (hidden_states,)
            past_key_value = past_key_values[idx] if past_key_values is not None else None
            layer_outputs = decoder_layer(
                hidden_states,
                position_embeddings=position_embeddings,
                attention_mask=attention_mask,
                past_key_value=past_key_value,
                use_cache=use_cache,
                **kwargs,
            )
            hidden_states = layer_outputs[0]
            if use_cache:
                next_decoder_cache += (layer_outputs[1],)
            if output_attentions:
                all_self_attns += (layer_outputs[1],)
        hidden_states = self.norm(hidden_states)
        # æ·»åŠ æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
        if output_hidden_states:
            all_hidden_states += (hidden_states,)
        # --- ä¿®å¤ç‚¹ 6: èšåˆå¹¶å­˜å‚¨ aux_loss ---
        # èšåˆæ‰€æœ‰ MoE å±‚çš„è¾…åŠ©æŸå¤±
        aux_loss = sum(
            layer.mlp.aux_loss
            for layer in self.layers
            if hasattr(layer.mlp, 'aux_loss') and layer.mlp.aux_loss is not None # æ£€æŸ¥æ˜¯å¦æ˜¯MoEå±‚ä¸”æœ‰æŸå¤±
        )
        # å°† aux_loss å­˜å‚¨ä¸ºæ¨¡å‹çš„ä¸€ä¸ªå±æ€§ï¼Œä¾› LexJadeForCausalLM è®¿é—®
        # æ³¨æ„ï¼šè¿™ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œä½†å¯¹äºå•GPUè®­ç»ƒæ˜¯å¯è¡Œçš„
        self._aux_loss = aux_loss
        # --- ä¿®å¤ç‚¹ 6 ç»“æŸ ---
        if not return_dict:
            return tuple(v for v in [hidden_states, next_decoder_cache, all_hidden_states, all_self_attns] if v is not None)
        # --- ä¿®å¤ç‚¹ 7: è¿”å› BaseModelOutputWithPast ---
        # LexJadeModel åº”è¯¥è¿”å› BaseModelOutputWithPastï¼Œ
        # å› ä¸ºå®ƒè¾“å‡ºçš„æ˜¯æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ (norm ä¹‹å)ï¼Œ
        # è€Œä¸æ˜¯æœ€ç»ˆçš„ logitsã€‚
        # logits çš„è®¡ç®—æ˜¯åœ¨ LexJadeForCausalLM ä¸­å®Œæˆçš„ã€‚
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states, # <-- è¿™æ˜¯æ­£ç¡®çš„å­—æ®µå
            past_key_values=next_decoder_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
            # aux_loss ä¸æ˜¯ BaseModelOutputWithPast çš„æ ‡å‡†å­—æ®µ
        )
        # --- ä¿®å¤ç‚¹ 7 ç»“æŸ ---
class LexJadeForCausalLM(PreTrainedModel, GenerationMixin):
    """
    LexJade å› æœè¯­è¨€æ¨¡å‹ï¼ŒåŒ…è£…ä¸»å¹²æ¨¡å‹å¹¶æ·»åŠ è¯­è¨€æ¨¡å‹å¤´ã€‚
    """
    config_class = LexJadeLM6Config
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config: LexJadeLM6Config):
        super().__init__(config)
        self.model = LexJadeModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        # ç»‘å®šæƒé‡ï¼šLMå¤´çš„æƒé‡ = åµŒå…¥å±‚çš„æƒé‡
        self.lm_head.weight = self.model.embed_tokens.weight
        # åˆå§‹åŒ–æƒé‡
        self.post_init()
    def get_input_embeddings(self):
        return self.model.embed_tokens
    def set_input_embeddings(self, value):
        self.model.embed_tokens = value
    def get_output_embeddings(self):
        return self.lm_head
    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings
    def set_decoder(self, decoder):
        self.model = decoder
    def get_decoder(self):
        return self.model
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        # --- ä¿®å¤ç‚¹ 8: è°ƒç”¨ä¸»å¹²æ¨¡å‹ ---
        # è°ƒç”¨ä¸»å¹²æ¨¡å‹
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            **kwargs
        )
        # hidden_states = outputs[0] # å¦‚æœ outputs æ˜¯å…ƒç»„
        # å¦‚æœ outputs æ˜¯ BaseModelOutputWithPast, hidden_states æ˜¯ .last_hidden_state
        # --- ä¿®å¤ç‚¹ 8 ç»“æŸ ---
        # --- ä¿®å¤ç‚¹ 9: ä»ä¸»å¹²æ¨¡å‹è·å–éšè—çŠ¶æ€ ---
        hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs.last_hidden_state
        # --- ä¿®å¤ç‚¹ 9 ç»“æŸ ---
        logits = self.lm_head(hidden_states)
        logits = logits.float() # è½¬ä¸º float32 ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
        loss = None
        if labels is not None:
            # è®¡ç®—å› æœè¯­è¨€å»ºæ¨¡æŸå¤±
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)
            # --- ä¿®å¤ç‚¹ 10: ä»ä¸»å¹²æ¨¡å‹è·å–å¹¶æ·»åŠ  MoE è¾…åŠ©æŸå¤± ---
            # æ·»åŠ  MoE è¾…åŠ©æŸå¤± (å¦‚æœå­˜åœ¨)
            # outputs.aux_loss å¯èƒ½ä¸å­˜åœ¨ (å› ä¸º BaseModelOutputWithPast æ²¡æœ‰)
            # æˆ‘ä»¬ä» self.model å®ä¾‹ä¸­è·å–
            model_aux_loss = getattr(self.model, '_aux_loss', None) # <-- ä¿®æ”¹ï¼šä» self.model è·å–
            if model_aux_loss is not None and isinstance(model_aux_loss, torch.Tensor) and model_aux_loss.requires_grad:
                 loss = loss + model_aux_loss
            # --- ä¿®å¤ç‚¹ 10 ç»“æŸ ---
        if not return_dict:
            output = (logits,) + outputs[1:] # (logits, past_key_values, hidden_states, attentions)
            # å¦‚æœéœ€è¦ï¼Œå¯ä»¥æ‰‹åŠ¨æ·»åŠ  aux_loss åˆ° output å…ƒç»„ä¸­ï¼Œä½†è¿™ä¸æ ‡å‡†
            # output = output + (model_aux_loss,) # ä¸æ¨è
            return (loss,) + output if loss is not None else output
        # --- ä¿®å¤ç‚¹ 11: è¿”å›æ ‡å‡†çš„ CausalLMOutputWithPast ---
        # ç¡®ä¿åªä¼ é€’ CausalLMOutputWithPast æ¥å—çš„å‚æ•°
        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            # aux_loss ä¸æ˜¯æ ‡å‡†å­—æ®µï¼Œå¦‚æœéœ€è¦ï¼Œè€ƒè™‘è‡ªå®šä¹‰è¾“å‡ºç±»æˆ–åœ¨ loss ä¸­åŒ…å«
        )
        # --- ä¿®å¤ç‚¹ 11 ç»“æŸ ---
    def prepare_inputs_for_generation(
        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs
    ):
        """
        ä¸ºç”Ÿæˆè¿‡ç¨‹å‡†å¤‡è¾“å…¥ã€‚
        """
        # å¤„ç†å·²ç”± past_key_values è¦†ç›–çš„token
        if past_key_values is not None:
            # past_key_values is a tuple of tuples/lists: (layer_0_past, layer_1_past, ...)
            # layer_i_past is (key_cache, value_cache), each tensor is [B, S_past, H_kv, D]
            past_length = past_key_values[0][0].shape[1] if past_key_values[0] is not None and len(past_key_values[0]) > 0 else 0
            # å¦‚æœ attention_mask æ¯” input_ids é•¿ï¼Œè¯´æ˜éƒ¨åˆ†è¾“å…¥æ˜¯é€šè¿‡ embeds ä¼ é€’çš„
            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:
                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]
            # å¦‚æœ past_length å°äº input_ids é•¿åº¦ï¼Œè¯´æ˜ input_ids åŒ…å«æ–°token
            elif past_length < input_ids.shape[1]:
                input_ids = input_ids[:, past_length:]
        position_ids = kwargs.get("position_ids", None)
        if attention_mask is not None and position_ids is None:
            # æ ¹æ® attention_mask åŠ¨æ€åˆ›å»º position_ids
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past_key_values:
                position_ids = position_ids[:, -input_ids.shape[1] :]
        # å¦‚æœä¼ å…¥äº† inputs_embeds ä¸”æ²¡æœ‰ past_key_valuesï¼Œä½¿ç”¨ embeds
        if inputs_embeds is not None and past_key_values is None:
            model_inputs = {"inputs_embeds": inputs_embeds}
        else:
            model_inputs = {"input_ids": input_ids}
        model_inputs.update(
            {
                "position_ids": position_ids,
                "past_key_values": past_key_values,
                "use_cache": kwargs.get("use_cache"),
                "attention_mask": attention_mask,
            }
        )
        return model_inputs
    @staticmethod
    def _reorder_cache(past_key_values, beam_idx):
        """
        ä¸º beam search é‡æ–°æ’åºç¼“å­˜ã€‚
        """
        reordered_past = ()
        for layer_past in past_key_values:
            reordered_past += (
                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),
            )
        return reordered_past