#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# @Time    : 2024/11/12 15:31
# @Author  : Han
# @File    : eval_model6.py
# @Software: VS Code
# @Desc    : è¯„ä¼° LexJadeLM6 ç³»åˆ—æ¨¡å‹çš„è„šæœ¬
#            æ”¯æŒå¤šç§æ¨¡å‹ç³»åˆ— (Tiny/Flash/Cube/Large/Extreme/Ultra/Extreme-M/Ultra-M)
#            æ”¯æŒè‡ªåŠ¨æ‰«ææ¨¡å‹ã€åŠ è½½ Hugging Face æ ¼å¼æ¨¡å‹ã€æ‰‹åŠ¨æŒ‡å®š .pth è·¯å¾„
#            æ”¯æŒ Pretrain å’Œ SFT/Chat æ¨¡å¼
#            æ”¯æŒè‡ªåŠ¨æµ‹è¯•å’Œæ‰‹åŠ¨è¾“å…¥æ¨¡å¼
#            æ”¯æŒå†å²å¯¹è¯ä¸Šä¸‹æ–‡ (ä»…é™ Chat æ¨¡å¼)
#            æ”¯æŒ TextStreamer (å·²å¯ç”¨)
import os
import sys
import re
import torch
import warnings
from transformers import AutoTokenizer, TextStreamer
# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ sys.path ä¸­ï¼Œä»¥ä¾¿æ­£ç¡®å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
    print(f"å·²å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path: {project_root}")
# - å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹å’Œé…ç½® -
try:
    from model.model_LexJadeLM6 import (
        LexJade6TinyConfig, LexJade6FlashConfig, LexJade6CubeConfig, LexJade6LargeConfig,
        LexJade6ExtremeConfig, LexJade6UltraConfig,
        LexJade6ExtremeMConfig, LexJade6UltraMConfig,
        LexJadeModel, LexJadeForCausalLM
    )
    print("æ¨¡å‹æ¨¡å—å¯¼å…¥æˆåŠŸã€‚")
except (ImportError, SyntaxError) as e:
    print(f"å¯¼å…¥æ¨¡å‹æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ 'model/model_LexJadeLM6.py' åœ¨æ­£ç¡®çš„è·¯å¾„ä¸‹ï¼Œå¹¶ä¸”æ²¡æœ‰è¯­æ³•é”™è¯¯ã€‚")
    exit(1)
# å¦‚æœä½ æœ‰ LoRA å·¥å…·å‡½æ•°ï¼Œè¯·ç¡®ä¿æ­£ç¡®å¯¼å…¥
# from model.model_lora import *
warnings.filterwarnings('ignore')
# - LexJadeLM6 æ¨¡å‹é…ç½®æ˜ å°„ -
MODEL_CONFIG_MAP = {
    "LexJade6-Tiny": LexJade6TinyConfig,
    "LexJade6-Flash": LexJade6FlashConfig,
    "LexJade6-Cube": LexJade6CubeConfig,
    "LexJade6-Large": LexJade6LargeConfig,
    "LexJade6-Extreme": LexJade6ExtremeConfig,
    "LexJade6-Ultra": LexJade6UltraConfig,
    "LexJade6-Extreme-M": LexJade6ExtremeMConfig,
    "LexJade6-Ultra-M": LexJade6UltraMConfig,
}

def get_base_out_dir():
    """è·å–åŸºç¡€æ¨¡å‹è¾“å‡ºç›®å½•"""
    while True:
        try:
            mode_choice = int(input("\n--- è¯·é€‰æ‹©æ¨¡å‹ç±»å‹ ---\n[0] Pretrain æ¨¡å‹ (out ç›®å½•)\n[1] SFT/Chat æ¨¡å‹ (sft_out ç›®å½•)\nè¯·è¾“å…¥é€‰é¡¹ (0 æˆ– 1): "))
            if mode_choice == 0:
                return "./out"
            elif mode_choice == 1:
                return "./sft_out"
            else:
                print("æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 0 æˆ– 1ã€‚")
        except ValueError:
            print("è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„æ•°å­—ã€‚")

def find_available_models(base_out_dir):
    """åœ¨æŒ‡å®šç›®å½•ä¸‹æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹æ£€æŸ¥ç‚¹"""
    available_models = []
    if not os.path.exists(base_out_dir):
        print(f"è­¦å‘Š: æ¨¡å‹è¾“å‡ºç›®å½• '{base_out_dir}' ä¸å­˜åœ¨ã€‚")
        return available_models
    # éå† base_out_dir ä¸‹çš„å­ç›®å½•ï¼ˆæ¯ä¸ªå­ç›®å½•å¯¹åº”ä¸€ä¸ªæ¨¡å‹ç³»åˆ—ï¼‰
    for model_series_dir in os.listdir(base_out_dir):
        series_path = os.path.join(base_out_dir, model_series_dir)
        if os.path.isdir(series_path):
            # åœ¨æ¯ä¸ªæ¨¡å‹ç³»åˆ—ç›®å½•ä¸‹æŸ¥æ‰¾ .pth æ–‡ä»¶
            for file in os.listdir(series_path):
                if file.endswith('.pth'):
                    full_path = os.path.join(series_path, file)
                    # å°è¯•ä»æ–‡ä»¶åæ¨æ–­æ¨¡å‹ç±»å‹å’Œæ¨¡å¼
                    # å‡è®¾æ–‡ä»¶åæ ¼å¼ç±»ä¼¼: LexJade6-Flash_epoch001_step01000.pth æˆ– final_LexJade6-Flash.pth
                    # æˆ–è€…åŒ…å« pretrain / sft ç­‰å…³é”®å­—
                    mode = "Unknown"
                    # æ ¹æ®ç›®å½•ååˆ¤æ–­æ¨¡å¼
                    if 'sft' in base_out_dir.lower():
                        mode = "SFT/Chat"
                    else: # 'out' ç›®å½•
                        mode = "Pretrain"
                    
                    # ä»æ–‡ä»¶åæˆ–ç›®å½•åä¸­æå–æ¨¡å‹ç³»åˆ—åç§°
                    model_series = None
                    # åŒ¹é…ç±»ä¼¼ LexJade6-Flash çš„æ¨¡å¼
                    match = re.search(r'(LexJade6-[A-Za-z0-9\-]+)', file)
                    if match:
                        model_series = match.group(1)
                    else:
                        # å¦‚æœæ–‡ä»¶åä¸åŒ…å«æ¨¡å‹ç³»åˆ—åï¼Œå°è¯•ä»ç›®å½•åè·å–
                        dir_match = re.search(r'(LexJade6-[A-Za-z0-9\-]+)', model_series_dir)
                        if dir_match:
                            model_series = dir_match.group(1)
                    if model_series and model_series in MODEL_CONFIG_MAP:
                        available_models.append({
                            'series': model_series,
                            'config_class': MODEL_CONFIG_MAP[model_series],
                            'path': full_path,
                            'mode': mode,
                            'filename': file
                        })
                    else:
                        print(f"è­¦å‘Š: æ— æ³•ä»æ–‡ä»¶å '{file}' æˆ–ç›®å½•å '{model_series_dir}' æ¨æ–­å‡ºæœ‰æ•ˆçš„æ¨¡å‹ç³»åˆ—ã€‚")
    return available_models

def load_model_from_ckpt(ckpt_path, config_class, device, mode="Pretrain"):
    """ä» .pth æ£€æŸ¥ç‚¹æ–‡ä»¶åŠ è½½æ¨¡å‹"""
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {ckpt_path}")
    print(f"æ¨¡å‹ç³»åˆ—: {config_class.__name__}, æ¨æ–­æ¨¡å¼: {mode}")
    try:
        config = config_class()
        model = LexJadeForCausalLM(config)
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(ckpt_path, map_location='cpu')
        state_dict = checkpoint
        # å¦‚æœæ£€æŸ¥ç‚¹æ˜¯å­—å…¸æ ¼å¼ï¼Œå°è¯•è·å– 'state_dict' æˆ– 'model' é”®
        if isinstance(checkpoint, dict):
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            elif 'model' in checkpoint:
                state_dict = checkpoint['model']
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé”®çš„æ£€æŸ¥
        # åŠ è½½çŠ¶æ€å­—å…¸
        # strict=False å…è®¸éƒ¨åˆ†åŒ¹é…ï¼Œè¿™å¯¹äºå¾®è°ƒæˆ–ç»“æ„ç•¥æœ‰ä¸åŒçš„æ¨¡å‹å¾ˆæœ‰ç”¨
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        if missing_keys:
            print(f"è­¦å‘Š: åŠ è½½æ—¶å‘ç°ç¼ºå¤±çš„é”®: {missing_keys}")
        if unexpected_keys:
            print(f"è­¦å‘Š: åŠ è½½æ—¶å‘ç°æ„å¤–çš„é”®: {unexpected_keys}")
        print("æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸã€‚")
        model.to(device)
        model.eval()
        # --- ä¿®æ”¹ç‚¹ 1: æ”¹è¿› Tokenizer æŸ¥æ‰¾é€»è¾‘ ---
        tokenizer = None
        # 1. ä»æ¨¡å‹è·¯å¾„æ¨æ–­æ¨¡å‹ç³»åˆ—åç§°
        model_series_name = None
        for series_name in MODEL_CONFIG_MAP.keys():
            # ä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…ï¼Œæ£€æŸ¥æ¨¡å‹ç³»åˆ—åæ˜¯å¦æ˜¯ ckpt_path çš„å­ä¸²
            if series_name in ckpt_path:
                model_series_name = series_name
                break
        # 2. æ„å»ºå¯èƒ½çš„ tokenizer è·¯å¾„åˆ—è¡¨
        # ä¼˜å…ˆæŸ¥æ‰¾ /root/minimind/model ä¸‹çš„ç‰¹å®š tokenizer å’Œå…¶å˜ä½“
        tokenizer_paths_to_check = []
        if model_series_name:
            # æ·»åŠ ç²¾ç¡®åŒ¹é…çš„è·¯å¾„ (ä¾‹å¦‚ tokenizer_LexJade6-Flash)
            tokenizer_paths_to_check.append(os.path.join("/root/minimind/model", f"tokenizer_{model_series_name}"))
            # æ·»åŠ å¯èƒ½çš„å˜ä½“è·¯å¾„ (ä¾‹å¦‚ tokenizer_LexJadeLM6-Flash)
            # è¿™æ˜¯ä¸ºäº†å…¼å®¹æ‚¨å®é™…çš„ç›®å½•å tokenizer_LexJadeLM6-Flash
            variant_name = model_series_name.replace("6", "LM6")
            if variant_name != model_series_name:
                tokenizer_paths_to_check.append(os.path.join("/root/minimind/model", f"tokenizer_{variant_name}"))
        # æ·»åŠ å…¶ä»–å¤‡é€‰è·¯å¾„
        tokenizer_paths_to_check.extend([
            os.path.join("/root/minimind/model", "tokenizer_LexJadeLM6"),          # é€šç”¨ tokenizer
            os.path.join(os.path.dirname(os.path.dirname(ckpt_path)), "tokenizer"), # åŒçº§ tokenizer ç›®å½•
            os.path.join(os.path.dirname(ckpt_path), "tokenizer"),                  # åŒçº§ tokenizer ç›®å½•
            os.path.join(project_root, "tokenizer"),                                # é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ tokenizer
            os.path.join(project_root, os.path.basename(os.path.dirname(ckpt_path)), "tokenizer"), # æ¨¡å‹ç³»åˆ—åä¸‹çš„ tokenizer
        ])
        # 3. éå†å€™é€‰è·¯å¾„å°è¯•åŠ è½½
        for path in tokenizer_paths_to_check:
            if os.path.exists(path):
                try:
                    tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)
                    print(f"Tokenizer ä» '{path}' åŠ è½½æˆåŠŸã€‚")
                    break
                except Exception as e:
                    print(f"å°è¯•ä» '{path}' åŠ è½½ tokenizer å¤±è´¥: {e}")
        # 4. å¦‚æœæ‰€æœ‰è·¯å¾„éƒ½å¤±è´¥ï¼Œåˆ™æŠ¥é”™
        if tokenizer is None:
            raise FileNotFoundError(
                f"æ— æ³•åœ¨ä»¥ä¸‹è·¯å¾„æ‰¾åˆ°åˆé€‚çš„ tokenizer: {tokenizer_paths_to_check}ã€‚"
                f"è¯·ç¡®ä¿å¯¹åº”æ¨¡å‹ç³»åˆ—çš„ tokenizer å­˜åœ¨äº '/root/minimind/model/' ç›®å½•ä¸‹ï¼Œ"
                f"ä¾‹å¦‚ '/root/minimind/model/tokenizer_LexJadeLM6-Flash' æˆ– '/root/minimind/model/tokenizer_LexJade6-Flash'ã€‚"
            )
        # --- ä¿®æ”¹ç‚¹ 1 ç»“æŸ ---
        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params / 1e6:.3f}M")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params / 1e6:.3f}M")
        return model, tokenizer
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def init_model(args):
    """åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨"""
    # --- å…³é”®ä¿®æ”¹: ç¡®ä¿ device åœ¨ init_model ä¸­ä¹Ÿè®¾ç½® ---
    if not hasattr(args, 'device') or args.device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        args.device = device
    # --- å…³é”®ä¿®æ”¹ç»“æŸ ---
    model = None
    tokenizer = None
    if args.load_mode == 0:
        # è‡ªåŠ¨æ‰«ææ¨¡å¼å·²åœ¨ main å‡½æ•°ä¸­å¤„ç†
        # è¿™é‡Œåº”è¯¥ä¸ä¼šè¢«è°ƒç”¨åˆ°ï¼Œä½†ä¸ºäº†å¥å£®æ€§ä¿ç•™
        print("é”™è¯¯ï¼šinit_model ä¸åº”åœ¨ load_mode=0 æ—¶è¢«ç›´æ¥è°ƒç”¨ã€‚")
        return None, None
    elif args.load_mode == 1:
        # åŠ è½½ Hugging Face æ ¼å¼æ¨¡å‹ (ç›®å‰æœªå®ç°ï¼Œå¯æ‰©å±•)
        print("load_mode=1 (Hugging Face æ ¼å¼) æš‚æœªå®ç°ã€‚")
        return None, None
    elif args.load_mode == 2:
        # æ‰‹åŠ¨æŒ‡å®š .pth è·¯å¾„
        ckpt_path = args.manual_ckpt_path
        if not os.path.exists(ckpt_path):
            print(f"æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {ckpt_path}")
            return None, None
        # ä»è·¯å¾„æ¨æ–­æ¨¡å‹ç³»åˆ—
        model_series = None
        for series_name in MODEL_CONFIG_MAP.keys():
            if series_name in ckpt_path:
                model_series = series_name
                break
        if not model_series:
            print(f"æ— æ³•ä»è·¯å¾„ '{ckpt_path}' æ¨æ–­å‡ºæ¨¡å‹ç³»åˆ—ã€‚è¯·ç¡®ä¿è·¯å¾„åŒ…å«æœ‰æ•ˆçš„æ¨¡å‹ç³»åˆ—åç§°ã€‚")
            return None, None
        config_class = MODEL_CONFIG_MAP[model_series]
        # æ¨æ–­æ¨¡å¼
        mode = "Pretrain" # é»˜è®¤
        if 'sft' in ckpt_path.lower() or 'chat' in ckpt_path.lower() or 'sft_out' in ckpt_path:
            mode = "SFT/Chat"
        model, tokenizer = load_model_from_ckpt(ckpt_path, config_class, args.device, mode)
    else:
        print(f"æ— æ•ˆçš„ --load_mode å€¼: {args.load_mode}")
    return model, tokenizer

def get_prompt_datas(args):
    """è·å–æµ‹è¯•æç¤ºæ•°æ®"""
    # å¯ä»¥ä»æ–‡ä»¶æˆ–ç›´æ¥å®šä¹‰
    default_prompts = [
        "é©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŸç†",
        "äººç±»å¤§è„‘çš„ä¸»è¦åŠŸèƒ½",
        "ä¸‡æœ‰å¼•åŠ›åŸç†æ˜¯",
        "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯",
        "äºŒæ°§åŒ–ç¢³åœ¨ç©ºæ°”ä¸­",
        "åœ°çƒä¸Šæœ€å¤§çš„åŠ¨ç‰©æœ‰",
        "æ­å·å¸‚çš„ç¾é£Ÿæœ‰"
    ]
    return default_prompts

def main():
    import argparse
    parser = argparse.ArgumentParser(description="Chat with LexJadeLM6")
    # - æ–°å¢/ä¿®æ”¹çš„å‚æ•° -
    parser.add_argument('--load_mode', default=0, type=int,
                        help="0: è‡ªåŠ¨æ‰«æå¹¶é€‰æ‹©æ¨¡å‹ (é»˜è®¤), 1: åŠ è½½ Hugging Face æ ¼å¼æ¨¡å‹, 2: æ‰‹åŠ¨æŒ‡å®š .pth è·¯å¾„")
    parser.add_argument('--manual_ckpt_path', default='', type=str,
                        help="å½“ load_mode=2 æ—¶ï¼ŒæŒ‡å®š .pth æ£€æŸ¥ç‚¹æ–‡ä»¶çš„å®Œæ•´è·¯å¾„")
    parser.add_argument('--history_cnt', default=0, type=int,
                        help="ä¿ç•™çš„å†å²å¯¹è¯è½®æ•° (é»˜è®¤ä¸º0ï¼Œå³ä¸ä¿ç•™å†å²)")
    parser.add_argument('--max_seq_len', default=512, type=int,
                        help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤512)")
    # - æ·»åŠ ç”Ÿæˆå‚æ•° -
    parser.add_argument('--temperature', type=float, default=0.5,
                        help="æ§åˆ¶éšæœºæ€§ï¼Œå€¼è¶Šå¤§è¾“å‡ºè¶Šéšæœº (0.1-2.0)")
    parser.add_argument('--top_p', type=float, default=0.9,
                        help="æ ¸é‡‡æ ·å‚æ•°ï¼Œç´¯ç§¯æ¦‚ç‡é˜ˆå€¼ (0.0-1.0)")
    parser.add_argument('--top_k', type=int, default=45,
                        help="é™åˆ¶è€ƒè™‘çš„æœ€é«˜æ¦‚ç‡è¯æ±‡æ•°é‡")
    parser.add_argument('--repetition_penalty', type=float, default=1.25,
                        help="é‡å¤æƒ©ç½šç³»æ•°ï¼Œå¤§äº1.0 (1.0-2.0)")
    parser.add_argument('--no_repeat_ngram_size', type=int, default=3,
                        help="é˜²æ­¢n-gramé‡å¤çš„å¤§å°")
    parser.add_argument('--min_new_tokens', type=int, default=1,
                        help="æœ€å°ç”Ÿæˆtokenæ•°")
    parser.add_argument('--early_stopping', action='store_true',
                        help="å½“ç”ŸæˆEOSæ—¶æå‰åœæ­¢")
    args = parser.parse_args()
    # - æ¨¡å‹åŠ è½½ -
    # --- ä¿®æ”¹: å…ˆé€‰æ‹©æ¨¡å‹ç±»å‹å†æ‰«æ ---
    base_out_dir = get_base_out_dir()
    # --- ä¿®æ”¹ç»“æŸ ---
    model = None
    tokenizer = None
    # --- å…³é”®ä¿®æ”¹: åœ¨æ‰€æœ‰åˆ†æ”¯ä¹‹å‰è®¾ç½® device ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    args.device = device
    # --- å…³é”®ä¿®æ”¹ç»“æŸ ---
    if args.load_mode == 0:
        # è‡ªåŠ¨æ‰«æå¹¶é€‰æ‹©æ¨¡å‹
        available_models = find_available_models(base_out_dir)
        if not available_models:
            print("åœ¨æŒ‡å®šç›®å½•ä¸‹æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚")
            return
        print("\n--- å¯ç”¨çš„æ¨¡å‹æ£€æŸ¥ç‚¹ ---")
        for i, model_info in enumerate(available_models):
            print(f"[{i}] {os.path.basename(model_info['path'])} (æ¨¡å¼: {model_info['mode']})")
        try:
            choice = int(input("\nè¯·é€‰æ‹©è¦åŠ è½½çš„æ¨¡å‹ç¼–å·: "))
            if 0 <= choice < len(available_models):
                selected_model_info = available_models[choice]
                # ä¿å­˜æ¨¡å¼ä¿¡æ¯ï¼Œä¾›åç»­ä½¿ç”¨
                args.model_mode_from_scan = selected_model_info['mode']
                model, tokenizer = load_model_from_ckpt(
                    selected_model_info['path'],
                    selected_model_info['config_class'],
                    args.device, # ä½¿ç”¨å·²è®¾ç½®çš„ device
                    selected_model_info['mode']
                )
            else:
                print("æ— æ•ˆçš„é€‰æ‹©ï¼Œç¨‹åºé€€å‡ºã€‚")
                return
        except ValueError:
            print("è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„æ•°å­—ï¼Œç¨‹åºé€€å‡ºã€‚")
            return
    elif args.load_mode in [1, 2]:
        model, tokenizer = init_model(args)
    else:
        print(f"æ— æ•ˆçš„ --load_mode å€¼: {args.load_mode}")
        return
    if model is None or tokenizer is None:
        print("æ¨¡å‹æˆ–åˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡ºã€‚")
        return
    # - æµ‹è¯•æµç¨‹ -
    prompts = get_prompt_datas(args)
    while True:
        try:
            test_mode = int(input('\n--- è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼ ---\n[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\nè¯·è¾“å…¥é€‰é¡¹ (0 æˆ– 1): '))
            if test_mode in [0, 1]:
                break
            else:
                print("æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 0 æˆ– 1ã€‚")
        except ValueError:
            print("è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„æ•°å­—ã€‚")
    # åˆå§‹åŒ–å¯¹è¯å†å²
    messages = []
    # åˆ¤æ–­æ˜¯å¦ä¸º Pretrain æ¨¡å¼
    # --- ä¿®æ”¹: æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ç±»å‹æˆ–æ‰«æç»“æœç¡®å®šæ¨¡å¼ ---
    is_pretrain_mode = getattr(args, 'model_mode_from_scan', 'Pretrain') == 'Pretrain'
    if args.load_mode == 2: # å¦‚æœæ˜¯æ‰‹åŠ¨åŠ è½½ï¼Œå†æ¬¡ç¡®è®¤æ¨¡å¼
         is_pretrain_mode = 'sft' not in args.manual_ckpt_path.lower() and 'chat' not in args.manual_ckpt_path.lower() and 'sft_out' not in args.manual_ckpt_path
    # --- ä¿®æ”¹ç»“æŸ ---
    print("\n--- å¼€å§‹æµ‹è¯• ---")
    if test_mode == 0:
        # è‡ªåŠ¨æµ‹è¯•æ¨¡å¼
        for prompt in prompts:
            print(f"\nğŸ‘¶: {prompt}")
            # - ä¿®å¤ç‚¹ 1: å†å²æ¶ˆæ¯å¤„ç† -
            if is_pretrain_mode:
                messages = [] # æ¸…ç©ºå†å²ï¼Œç¡®ä¿ç‹¬ç«‹ç”Ÿæˆ
            else:
                if args.history_cnt > 0:
                    messages = messages[-args.history_cnt:]
                else:
                    messages = []
            # - ä¿®å¤ç‚¹ 1 ç»“æŸ -
            messages.append({"role": "user", "content": prompt})
            # - ä¿®å¤ç‚¹ 2: ç®€åŒ–æ¨¡å¼åˆ¤æ–­å’Œè¾“å…¥æ„å»º -
            if not is_pretrain_mode:
                try:
                    new_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                except Exception as e:
                    print(f"åº”ç”¨èŠå¤©æ¨¡æ¿å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨ç®€å•æ‹¼æ¥ã€‚")
                    new_prompt = ""
                    for msg in messages:
                        if msg["role"] == "user":
                            new_prompt += f"<|user|>{msg['content']}"
                        elif msg["role"] == "assistant":
                            new_prompt += f"<|assistant|>{msg['content']}"
                    new_prompt += "<|assistant|>"
            else:
                # Pretrain æ¨¡å¼ä½¿ç”¨æœ€ç®€å•çš„å½¢å¼
                new_prompt = prompt # tokenizer.bos_token + prompt
            # - ä¿®å¤ç‚¹ 2 ç»“æŸ -
            inputs = tokenizer(new_prompt, return_tensors="pt", truncation=True, max_length=args.max_seq_len).to(args.device)
            if inputs["input_ids"].size(1) == args.max_seq_len:
                print("è­¦å‘Š: è¾“å…¥å·²è¾¾åˆ°æœ€å¤§é•¿åº¦é™åˆ¶ã€‚")
            print('ğŸ¤–ï¸: ', end='')

            # --- å¯ç”¨æµå¼è¾“å‡º ---
            # åˆ›å»º TextStreamer å®ä¾‹
            # æ³¨æ„ï¼šskip_prompt=True é¿å…æ‰“å°è¾“å…¥æç¤ºè¯, skip_special_tokens=True ç§»é™¤ç‰¹æ®Štoken
            streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
            # --- å¯ç”¨æµå¼è¾“å‡ºç»“æŸ ---

            # æ¨¡å‹ç”Ÿæˆ
            try:
                with torch.no_grad():
                    # --- ä¿®æ”¹ generate è°ƒç”¨ä»¥å¯ç”¨æµå¼è¾“å‡º ---
                    # ç§»é™¤æˆ–è°ƒæ•´ max_new_tokens çš„è®¡ç®—æ–¹å¼ï¼Œç¡®ä¿é€»è¾‘å…¼å®¹
                    generated_ids = model.generate(
                        inputs["input_ids"],
                        # max_new_tokens=min(512, args.max_seq_len - inputs["input_ids"].size(1)), # å¯ä»¥ä¿ç•™
                        max_new_tokens=args.max_seq_len - inputs["input_ids"].size(1) if inputs["input_ids"].size(1) < args.max_seq_len else 512, # ç¡®ä¿ max_new_tokens > 0
                        num_return_sequences=1,
                        do_sample=True,
                        temperature=args.temperature,
                        top_p=args.top_p,
                        top_k=args.top_k,
                        repetition_penalty=args.repetition_penalty,
                        no_repeat_ngram_size=args.no_repeat_ngram_size,
                        min_new_tokens=args.min_new_tokens,
                        early_stopping=args.early_stopping,
                        attention_mask=inputs["attention_mask"],
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        streamer=streamer, # <-- å¯ç”¨æµå¼è¾“å‡º
                    )
                    # --- æµå¼è¾“å‡ºä¸‹ï¼Œåç»­å¤„ç†éœ€è¦è°ƒæ•´ ---
                    # å› ä¸º TextStreamer å·²ç»æ‰“å°äº†è¾“å‡ºï¼Œé€šå¸¸ä¸éœ€è¦å† decode å’Œ print response
                    # ä½†å¦‚æœä½ éœ€è¦å°† response å­˜å…¥ messages æˆ–åšå…¶ä»–å¤„ç†ï¼Œä»éœ€ decode

                    # --- ä¿ç•™è°ƒè¯•ä¿¡æ¯ (å¯é€‰) ---
                    # print(f"DEBUG: inputs['input_ids'].shape = {inputs['input_ids'].shape}")
                    # print(f"DEBUG: inputs['input_ids'].dtype = {inputs['input_ids'].dtype}")
                    # print(f"DEBUG: generated_ids.shape = {generated_ids.shape}")
                    # print(f"DEBUG: generated_ids.dtype = {generated_ids.dtype}")

                    # --- ä¿ç•™å†å²è®°å½•å’Œ decode (å¦‚æœéœ€è¦) ---
                    # æ£€æŸ¥ç»´åº¦å’Œé•¿åº¦ (è¿™éƒ¨åˆ†é€»è¾‘å¯èƒ½éœ€è¦æ ¹æ® streamer çš„è¡Œä¸ºå¾®è°ƒï¼Œä½†åŸºç¡€æ£€æŸ¥ä¿ç•™)
                    if generated_ids.dim() != 2:
                         # å¯ä»¥æ”¹ä¸ºè­¦å‘Šï¼Œå› ä¸º streamer å¯èƒ½æ”¹å˜äº†è¡Œä¸ºï¼Œæˆ–è€…ç¡®ä¿ generate æ­£å¸¸è¿”å›
                         print(f"Warning: Expected generated_ids to be 2-dimensional, but got {generated_ids.dim()}-dimensional tensor with shape {generated_ids.shape}. Streamer might be in use.")
                         # ä¸ºäº†åç»­å¤„ç†ï¼Œå‡è®¾ç»´åº¦æ˜¯æ­£ç¡®çš„æˆ–å°è¯•æ¢å¤
                         if generated_ids.dim() == 1:
                             generated_ids = generated_ids.unsqueeze(0) # å°è¯•è°ƒæ•´ä¸º [1, seq_len]
                         else:
                             response = "<ç”Ÿæˆé”™è¯¯: å¼ é‡ç»´åº¦å¼‚å¸¸>"
                             if not is_pretrain_mode:
                                 messages.append({"role": "assistant", "content": response})
                             # print(response) # å¦‚æœ decode å¤±è´¥ï¼Œæ‰“å°é”™è¯¯ (éæµå¼)
                             print('\n' + '-' * 20)
                             continue # è·³è¿‡æœ¬è½®å¾ªç¯çš„å‰©ä½™éƒ¨åˆ†

                    input_len = inputs["input_ids"].shape[1]
                    total_len = generated_ids.shape[1]

                    if input_len > total_len:
                        print(f"Warning: Input length ({input_len}) seems greater than generated total length ({total_len}). Streamer might be in use or generation failed.")
                        response = "<ç”Ÿæˆé”™è¯¯: é•¿åº¦å¼‚å¸¸>"
                        if not is_pretrain_mode:
                            messages.append({"role": "assistant", "content": response})
                        # print(response) # æ‰“å°é”™è¯¯ä¿¡æ¯ (éæµå¼)
                        print('\n' + '-' * 20)
                        continue
                    elif input_len == total_len:
                        # print("Warning: Input length equals total generated length. No new tokens generated?") # è­¦å‘Šå¯é€‰
                        response = "" # ç©ºå“åº”
                    else:
                        # å®‰å…¨åœ°åˆ‡ç‰‡å¹¶è§£ç 
                        generated_token_ids = generated_ids[0, input_len:] # Shape: [new_sequence_length]
                        # æ³¨æ„ï¼šå³ä½¿ä½¿ç”¨äº† streamerï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥ decode æ¥è·å–å®Œæ•´çš„ response å­—ç¬¦ä¸²ç”¨äºå†å²è®°å½•
                        response = tokenizer.decode(generated_token_ids, skip_special_tokens=True)

                    # --- å°†å“åº”æ·»åŠ åˆ°å†å²è®°å½• (å¦‚æœé€‚ç”¨) ---
                    if not is_pretrain_mode:
                        messages.append({"role": "assistant", "content": response})
                    # --- æµå¼è¾“å‡ºä¸‹ï¼Œé€šå¸¸ä¸åœ¨æ­¤å¤„æ‰“å° response ---
                    # print(response, end='') # <-- æ³¨é‡Šæ‰æˆ–åˆ é™¤è¿™è¡Œ

            except Exception as e:
                print(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                response = "<ç”Ÿæˆé”™è¯¯>"
                # å³ä½¿å‡ºé”™ï¼Œä¹Ÿæ‰“å°é”™è¯¯ä¿¡æ¯ (éæµå¼)
                # print(response, end='')

            # --- åœ¨æµå¼è¾“å‡ºåï¼Œæ‰“å°åˆ†éš”ç¬¦ ---
            print('\n' + '-' * 20) # åœ¨æ¯æ¬¡å“åº”åæ‰“å°åˆ†éš”ç¬¦

    else:
        # æ‰‹åŠ¨è¾“å…¥æ¨¡å¼
        while True:
            try:
                user_input = input("\nğŸ‘¶: ")
                if user_input.lower() in ['é€€å‡º', 'exit', 'quit']:
                    print("å†è§ï¼")
                    break
                prompt = user_input
                # - ä¿®å¤ç‚¹ 1: å†å²æ¶ˆæ¯å¤„ç† (åŒä¸Š) -
                if is_pretrain_mode:
                    messages = []
                else:
                    if args.history_cnt > 0:
                        messages = messages[-args.history_cnt:]
                    else:
                        messages = []
                # - ä¿®å¤ç‚¹ 1 ç»“æŸ -
                messages.append({"role": "user", "content": prompt})
                # - ä¿®å¤ç‚¹ 2: ç®€åŒ–æ¨¡å¼åˆ¤æ–­å’Œè¾“å…¥æ„å»º (åŒä¸Š) -
                if not is_pretrain_mode:
                    try:
                        new_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                    except Exception as e:
                        print(f"åº”ç”¨èŠå¤©æ¨¡æ¿å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨ç®€å•æ‹¼æ¥ã€‚")
                        new_prompt = ""
                        for msg in messages:
                            if msg["role"] == "user":
                                new_prompt += f"<|user|>{msg['content']}"
                            elif msg["role"] == "assistant":
                                new_prompt += f"<|assistant|>{msg['content']}"
                        new_prompt += "<|assistant|>"
                else:
                    new_prompt = prompt
                # - ä¿®å¤ç‚¹ 2 ç»“æŸ -
                inputs = tokenizer(new_prompt, return_tensors="pt", truncation=True, max_length=args.max_seq_len).to(args.device)
                if inputs["input_ids"].size(1) == args.max_seq_len:
                    print("è­¦å‘Š: è¾“å…¥å·²è¾¾åˆ°æœ€å¤§é•¿åº¦é™åˆ¶ã€‚")
                print('ğŸ¤–ï¸: ', end='')

                 # --- å¯ç”¨æµå¼è¾“å‡º ---
                streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
                # --- å¯ç”¨æµå¼è¾“å‡ºç»“æŸ ---

                # æ¨¡å‹ç”Ÿæˆ
                try:
                    with torch.no_grad():
                        # --- ä¿®æ”¹ generate è°ƒç”¨ä»¥å¯ç”¨æµå¼è¾“å‡º ---
                        generated_ids = model.generate(
                            inputs["input_ids"],
                            max_new_tokens=args.max_seq_len - inputs["input_ids"].size(1) if inputs["input_ids"].size(1) < args.max_seq_len else 512,
                            num_return_sequences=1,
                            do_sample=True,
                            temperature=args.temperature,
                            top_p=args.top_p,
                            top_k=args.top_k,
                            repetition_penalty=args.repetition_penalty,
                            no_repeat_ngram_size=args.no_repeat_ngram_size,
                            min_new_tokens=args.min_new_tokens,
                            early_stopping=args.early_stopping,
                            attention_mask=inputs["attention_mask"],
                            pad_token_id=tokenizer.pad_token_id,
                            eos_token_id=tokenizer.eos_token_id,
                            streamer=streamer, # <-- å¯ç”¨æµå¼è¾“å‡º
                        )

                        # --- ä¿ç•™å†å²è®°å½•å’Œ decode (å¦‚æœéœ€è¦) ---
                        if generated_ids.dim() != 2:
                             print(f"Warning: Expected generated_ids to be 2-dimensional, but got {generated_ids.dim()}-dimensional tensor with shape {generated_ids.shape}. Streamer might be in use.")
                             if generated_ids.dim() == 1:
                                 generated_ids = generated_ids.unsqueeze(0)
                             else:
                                 response = "<ç”Ÿæˆé”™è¯¯: å¼ é‡ç»´åº¦å¼‚å¸¸>"
                                 if not is_pretrain_mode:
                                     messages.append({"role": "assistant", "content": response})
                                 # print(response) # æ‰“å°é”™è¯¯ä¿¡æ¯ (éæµå¼)
                                 print('\n' + '-' * 20)
                                 continue

                        input_len = inputs["input_ids"].shape[1]
                        total_len = generated_ids.shape[1]

                        if input_len > total_len:
                            print(f"Warning: Input length ({input_len}) seems greater than generated total length ({total_len}). Streamer might be in use or generation failed.")
                            response = "<ç”Ÿæˆé”™è¯¯: é•¿åº¦å¼‚å¸¸>"
                            if not is_pretrain_mode:
                                messages.append({"role": "assistant", "content": response})
                            # print(response) # æ‰“å°é”™è¯¯ä¿¡æ¯ (éæµå¼)
                            print('\n' + '-' * 20)
                            continue
                        elif input_len == total_len:
                            # print("Warning: Input length equals total generated length. No new tokens generated?") # è­¦å‘Šå¯é€‰
                            response = ""
                        else:
                            generated_token_ids = generated_ids[0, input_len:]
                            response = tokenizer.decode(generated_token_ids, skip_special_tokens=True)

                        if not is_pretrain_mode:
                            messages.append({"role": "assistant", "content": response})
                        # print(response, end='') # <-- æ³¨é‡Šæ‰æˆ–åˆ é™¤è¿™è¡Œ

                except Exception as e:
                    print(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    response = "<ç”Ÿæˆé”™è¯¯>"
                    # print(response, end='') # æ‰“å°é”™è¯¯ä¿¡æ¯ (éæµå¼)

                print('\n' + '-' * 20) # åœ¨æ¯æ¬¡å“åº”åæ‰“å°åˆ†éš”ç¬¦
            except KeyboardInterrupt:
                print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œé€€å‡ºç¨‹åºã€‚")
                break
            except Exception as e:
                print(f"\nå‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()

if __name__ == "__main__":
    main()